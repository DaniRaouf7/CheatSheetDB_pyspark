{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca41ea28-c931-4432-a927-80dfeca45b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb7ff64-def7-4fd0-9269-e05c8d1321a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432bbcd2-8a97-41e5-9c44-d96f3614b867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DB utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306a16d2-c491-4f7e-8d57-6ffa59cc570f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Which Files?\n",
    "dbutils.fs.ls(\"/FileStore/tables/\")\n",
    "\n",
    "dbutils.widgets.text(\"path\", \"abfss://dbmasterclass@storageaccnetflixdr.dfs.core.windows.net/source\")\n",
    "path = dbutils.widgets.get(\"path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c391781-7823-46d7-a5dd-fef3489a048a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Storage Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29f17a89-d57d-453c-b8f5-b98f125ff9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "abfss://bronze@<container-name>.dfs.core.windows.net/folder1/folder2\n",
    "\n",
    "dbutils.widgets.text(\"path\", \"abfss://dbmasterclass@storageaccnetflixdr.dfs.core.windows.net/source\")\n",
    "path = dbutils.widgets.get(\"path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91170b1-462d-4480-b6e1-34ef8a0a180d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading different file formats & Writing :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "934f843b-90d0-47c5-86de-25f46db94387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a7e5e1-7d11-49c4-a892-d59bff152d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV\n",
    "df_csv = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"/FileStore/tables/BigMart_Sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01575cc7-d0fe-4bd1-b98f-f57a0a64a63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JSON\n",
    "df_json = spark.read.format(\"json\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"multiLine\", \"false\")\\\n",
    "    .load(\"/FileStore/tables/drivers.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "936c8660-041a-49a6-a489-c2c715de2786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd20f8f0-12ab-44c2-b8e3-d05de261fb63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to Delta/ csv/ parquet\n",
    "df_csv.write.format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save(\"/FileStore/tables/delta/data.delta\")\n",
    "\n",
    "# Append overwrite error ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8756e865-505f-42ff-aebd-d48668de9ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define own schema?!? DDL ofcourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90aac930-c7a5-437c-858e-e3470ff1d665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print schema for copilot, recognize it and will autocorrect code :)\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a038c7d4-0470-45a8-a676-cc8a2a29209f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DDL duh\n",
    "\n",
    "ddl_schema = \"\"\" \n",
    "                Item_Identifier STRING,\n",
    "                Item_Weight STRING,\n",
    "                Item_Fat_Content STRING,\n",
    "                Item_Visibility DOUBLE,\n",
    "                Item_Type STRING,\n",
    "                Item_MRP DOUBLE,\n",
    "                Outlet_Identifier STRING,\n",
    "                Outlet_Establishment_Year INTEGER,\n",
    "                Outlet_Size STRING,\n",
    "                Outlet_Location_Type STRING,\n",
    "                Outlet_Type STRING,\n",
    "                Item_Outlet_Sales DOUBLE\n",
    "\"\"\"\n",
    "\n",
    "df_test = spark.read.format(\"csv\")\\\n",
    "     .option(\"header\", \"true\")\\\n",
    "     .schema(ddl_schema)\\\n",
    "     .load(\"/FileStore/tables/BigMart_Sales.csv\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fd2552e8-8800-45a9-9c2c-4f9fd45c50a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f5937d-ac4f-416b-95f6-9d0322f8cd79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c317fd62-def3-4129-94a1-1699d1e33a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df_csv.select(col(\"item_identifier\"), col(\"item_weight\"), col(\"item_fat_content\"))\n",
    "\n",
    "# Limit function\n",
    "df_csv.limit(10)    \n",
    "\n",
    "# Renaming\n",
    "# alias() verandert de naam alleen binnen de select\n",
    "df_csv.select(col(\"item_identifier\").alias(\"AMK\"), col(\"Item_MRP\"))\n",
    "\n",
    "# Binnen de DF echt zelf\n",
    "df_csv.withColumnRenamed(\"item_identifier\", \"Item_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d683d6-b216-48ac-860c-16e4c64d7607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c716103-20db-4f4e-97af-21652b1c8288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter/ Where\n",
    "# Single Filter\n",
    "df_csv.filter(col(\"item_identifier\") == \"FDA15\")\n",
    "\n",
    "# Double Filter\n",
    "df_csv.filter((col(\"item_identifier\") == \"FDA15\") & (col(\"outlet_size\") == \"Small\"))\n",
    "\n",
    "# Triple or with and combined\n",
    "df_test.filter((col(\"Outlet_Location_Type\").isin(\"Tier 1\", \"Tier 2\")) & (col(\"Outlet_Size\").isNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc82b00e-4e5d-4dcb-a09c-50fa9c7fb2da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Modify and Create new Column + aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f5f385-7bd6-4a44-8c22-690db40c115c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adding constant value\n",
    "df_csv.withColumn(\"Flag\", lit(\"new\"))\n",
    "\n",
    "# Multiply\n",
    "df_csv.withColumn(\"Item_Weight_multiplied\", col(\"item_weight\") * col(\"item_mrp\"))\n",
    "\n",
    "# Modify same column\n",
    "df_csv.withColumn(\"item_fat_content\", regexp_replace(col(\"item_fat_content\"), \"Regular\", \"Reg\"))\\\n",
    "    .withColumn(\"item_fat_content\", regexp_replace(col(\"item_fat_content\"), \"Low Fat\", \"LF\"))\n",
    "\n",
    "# Type casting\n",
    "df_csv.withColumn(\"item_weight\", col(\"item_weight\").cast(\"string\"))\n",
    "\n",
    "# Sort/Orderby asc() desc() moet aan de column als methode worden gezet\n",
    "df_csv.sort(col(\"item_weight\").desc())\n",
    "df_csv.sort(col(\"item_visibility\").asc())\n",
    "df_csv.sort([col(\"item_weight\").desc(), col(\"item_visibility\").asc()])\n",
    "\n",
    "# Group_by with sum + other scenarios\n",
    "df_csv.groupBy(\"item_type\").sum(\"item_mrp\")\n",
    "df_csv.groupBy(\"item_type\").avg(\"item_mrp\")\n",
    "df_csv.groupBy(\"item_type\").count()\n",
    "df_csv.groupBy(\"item_type\", \"outlet_size\").sum(\"item_mrp\")\\\n",
    "    .withColumnRenamed(\"sum(item_mrp)\", \"total_mrp\")\\\n",
    "    .orderBy(\"item_type\")\n",
    "df_csv.groupBy(\"item_type\", \"outlet_size\")\\\n",
    "    .agg(sum(\"item_mrp\").alias(\"total_mrp\"), avg(\"item_mrp\").alias(\"avg_mrp\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ede66d-b98e-4122-be42-3cc6aba0f3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4fd8ee-cb07-45a7-aed7-a3d58fc7e2bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "df_csv.drop(\"item_weight\")\n",
    "\n",
    "# Dropping multiple columns\n",
    "df_csv.drop(\"item_weight\", \"item_mrp\")\n",
    "\n",
    "# dropping Null values? fillna\n",
    "df_csv.na.drop('any', subset=[\"item_weight\"]) # 'all' ( de gehele rij null) of 'any' (alle rijen met null in any kolom)\n",
    "df_csv.fillna(\"0\", \"item_weight\")\n",
    "\n",
    "# Dropping duplicates\n",
    "df_csv.dropDuplicates([\"item_identifier\"])\n",
    "df_csv.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97974c60-3ee9-47c7-ac9d-9f2c4d9bfbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### String manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aae25bc-f9d7-4b72-b894-9d7df6596b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initcap, lower, upper\n",
    "df_csv.withColumn(\"Item_fat_content\", upper(col(\"item_fat_content\")))\n",
    "df_csv.withColumn(\"Item_fat_content\", lower(col(\"item_fat_content\")))\n",
    "df_csv.withColumn(\"Item_fat_content\", initcap(col(\"item_fat_content\"))) # Hoofdletter bij eerste letter van het woord\n",
    "\n",
    "# Split & indexing\n",
    "df_csv.withColumn(\"item_fat_content\", split(col(\"item_fat_content\"), \" \")[1]) # Split op spatie ( creert een list) en selecteer de tweede string\n",
    "\n",
    "# Explode :)\n",
    "df_csv.withColumn(\"item_fat_content\", explode(split(col(\"item_fat_content\"), \" \"))) # Split op spatie en explode de list maakt voor de hele lijst nieuwe rijen\n",
    "\n",
    "# Collect list, maakt een list van een bepaalde kolom op basis van een ander kolom\n",
    "df_csv.groupBy(\"user_id\").agg(collect_list(\"book\"))\n",
    "\n",
    "# Array_contains -> get a boolean value in new column\n",
    "df_csv.withColumn(\"Flag\", array_contains(col(\"item_fat_content\"), \"LF\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4290f650-b29e-48cc-9044-d97e5bed1a0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Date functions, VERY important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a36cff-ac35-4b72-a41c-60b7fa471af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Huidige Datum & Tijd\n",
    "df_csv.withColumn(\"current_date\", current_date())  # Geeft de huidige datum terug (YYYY-MM-DD)\n",
    "df_csv.withColumn(\"current_timestamp\", current_timestamp())  # Geeft de huidige datum + tijd (YYYY-MM-DD HH:MM:SS)\n",
    "\n",
    "# Datum Omzetten naar String & Andersom\n",
    "df_csv.withColumn(\"date_string\", date_format(col(\"datum\"), \"yyyy/MM/dd\"))  # Converteert datum naar string in een bepaald formaat dd/MM/yyyy\n",
    "df_csv.withColumn(\"to_date\", to_date(col(\"datum\")))  # Converteert een string naar een datumtype\n",
    "df_csv.withColumn(\"to_timestamp\", to_timestamp(col(\"datum\")))  # Converteert een string naar een timestamp\n",
    "\n",
    "# Datum Berekeningen\n",
    "df_csv.withColumn(\"plus_5_days\", date_add(col(\"datum\"), 5))  # Voegt 5 dagen toe aan een datum\n",
    "df_csv.withColumn(\"minus_5_days\", date_sub(col(\"datum\"), 5))  # Trekt 5 dagen af van een datum\n",
    "df_csv.withColumn(\"next_month\", add_months(col(\"datum\"), 1))  # Voegt 1 maand toe aan een datum\n",
    "df_csv.withColumn(\"prev_month\", add_months(col(\"datum\"), -1))  # Trekt 1 maand af van een datum\n",
    "df_csv.withColumn(\"date_diff\", datediff(col(\"eind_datum\"), col(\"start_datum\")))  # Aantal dagen tussen twee datums\n",
    "df_csv.withColumn(\"months_between\", months_between(col(\"eind_datum\"), col(\"start_datum\")))  # Aantal maanden tussen twee datums\n",
    "df_csv.withColumn(\"last_day_of_month\", last_day(col(\"datum\")))  # Geeft de laatste dag van de maand terug\n",
    "\n",
    "# Weekdag, Maand & Jaar Ophalen\n",
    "df_csv.withColumn(\"jaar\", year(col(\"datum\")))  # Geeft het jaar terug\n",
    "df_csv.withColumn(\"maand\", month(col(\"datum\")))  # Geeft de maand terug\n",
    "df_csv.withColumn(\"dag\", dayofmonth(col(\"datum\")))  # Geeft de dag van de maand terug\n",
    "df_csv.withColumn(\"dag_van_week\", dayofweek(col(\"datum\")))  # Geeft de dag van de week (1=Zondag, 7=Zaterdag)\n",
    "df_csv.withColumn(\"dag_van_jaar\", dayofyear(col(\"datum\")))  # Geeft de dag van het jaar terug (1-365)\n",
    "df_csv.withColumn(\"weeknummer\", weekofyear(col(\"datum\")))  # Geeft het weeknummer van het jaar terug\n",
    "\n",
    "# Tijd (Uren, Minuten, Seconden)\n",
    "df_csv.withColumn(\"uur\", hour(col(\"timestamp\")))  # Geeft het uur terug\n",
    "df_csv.withColumn(\"minuut\", minute(col(\"timestamp\")))  # Geeft de minuten terug\n",
    "df_csv.withColumn(\"seconde\", second(col(\"timestamp\")))  # Geeft de seconden terug\n",
    "\n",
    "# Unix Timestamp & Epoch Tijd\n",
    "df_csv.withColumn(\"unix_timestamp\", unix_timestamp(col(\"datum\"), \"yyyy-MM-dd\"))  # Converteert datum naar een Unix timestamp\n",
    "df_csv.withColumn(\"from_unix\", from_unixtime(col(\"unix_timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))  # Converteert Unix timestamp naar datum\n",
    "\n",
    "# Begin en Einde van Periodes\n",
    "df_csv.withColumn(\"begin_van_maand\", trunc(col(\"datum\"), \"month\"))  # Geeft de eerste dag van de maand\n",
    "df_csv.withColumn(\"begin_van_jaar\", trunc(col(\"datum\"), \"year\"))  # Geeft de eerste dag van het jaar\n",
    "df_csv.withColumn(\"begin_van_week\", date_trunc(\"week\", col(\"datum\")))  # Geeft de eerste dag van de week\n",
    "df_csv.withColumn(\"begin_van_dag\", date_trunc(\"day\", col(\"timestamp\")))  # Geeft de begindatum van de dag\n",
    "df_csv.withColumn(\"begin_van_uur\", date_trunc(\"hour\", col(\"timestamp\")))  # Geeft de begindatum van het uur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f74cb9b-1a86-4100-a86f-44a45d8fe6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Joins & Unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c226315-7f9b-42cb-9555-39b850aad8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How works UNION? Only when tables are the same schema and data types of course\n",
    "df_csv.union(df_json)\n",
    "df_csv.unionByName(df_json)\n",
    "\n",
    "# Inner Join (Standaard) - Houdt alleen rijen die in beide DataFrames voorkomen\n",
    "df_csv.join(df_json, \"driverId\")  # Zelfde als \"inner\" / Of je schrijft df_csv.join(df_json, df_csv[\"name_Id\"] == df_json[\"driverId\"])\n",
    "df_csv.join(df_json, df_csv[\"name_Id\"] == df_json[\"driverId\"], \"inner\") # je kan ze joinen maar namen zijn anders\n",
    "\n",
    "# Left Join (Linker Outer Join) - Houdt alle rijen uit het linker DataFrame, en alleen overeenkomende rijen uit het rechter\n",
    "df_csv.join(df_json, \"driverId\", \"left\")\n",
    "df_csv.join(df_json, \"driverId\", \"left_outer\")  # \"left\" en \"left_outer\" zijn hetzelfde\n",
    "\n",
    "# Right Join (Rechter Outer Join) - Houdt alle rijen uit het rechter DataFrame, en alleen overeenkomende rijen uit het linker ( Wie gebruikt dit eigenlijk?!?!)\n",
    "df_csv.join(df_json, \"driverId\", \"right\")  \n",
    "df_csv.join(df_json, \"driverId\", \"right_outer\")  # \"right\" en \"right_outer\" zijn hetzelfde\n",
    "\n",
    "# Full Outer Join - Houdt alle rijen uit beide DataFrames, vult NULLs in waar geen match is\n",
    "df_csv.join(df_json, \"driverId\", \"full\")\n",
    "df_csv.join(df_json, \"driverId\", \"full_outer\")  # \"full\" en \"full_outer\" zijn hetzelfde\n",
    "df_csv.join(df_json, \"driverId\", \"outer\")  # Zelfde als full outer join\n",
    "\n",
    "# Left Semi Join - Houdt alleen rijen uit het linker DataFrame die een match hebben in het rechter\n",
    "df_csv.join(df_json, \"driverId\", \"left_semi\")  \n",
    "\n",
    "# Left Anti Join - Houdt alleen rijen uit het linker DataFrame die GEEN match hebben in het rechter\n",
    "df_csv.join(df_json, \"driverId\", \"left_anti\") \n",
    "\n",
    "# Cross Join - Combineert elke rij van het linker DataFrame met elke rij van het rechter (cartesisch product)\n",
    "df_csv.crossJoin(df_json)  \n",
    "\n",
    "# Self Join - Joins een DataFrame met zichzelf (bijv. voor hiÃ«rarchische data of gerelateerde records)\n",
    "df_csv.join(df_csv, \"item_identifier\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f1bc82-b4da-445e-8a5d-06a395497777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### When - otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc187f0e-8fd1-4d7d-8d38-97497bd6eb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# conditional functions\n",
    "df_csv.withColumn(\"veg_flag\", when(col(\"item_type\") == \"Meat\", \"non-veg\").otherwise(\"veg\"))\n",
    "df_csv.withColumn(\"exp_veg_flag\", when((col(\"item_type\") == \"Fruits and Vegetables\") & (col(\"item_mrp\") > 100), \"expensive\")\\\n",
    "    .when((col(\"item_type\") == \"Fruits and Vegetables\") & (col(\"item_mrp\") < 100), \"in_expensive\")\\\n",
    "    .otherwise(\"non-veg-type\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb581ee-0426-41e1-8078-017c8dd40c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Pivot _tables_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567d3775-c11f-4dde-a2e0-d20e11f4a043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pivot (rijen naar kolommen)\n",
    "df_csv.groupBy(\"Item_type\").pivot(\"Outlet_Size\").agg(avg('item_mrp')) # Groep bij item type, outlet size worden kolommen gevuld met avg(item_mrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f73e608-e6e7-4252-8f58-764fb1a8bfdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5981ea63-afed-4200-a0b4-6ee4e01b0acb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Elke rij een rijnummer geven\n",
    "df_csv.withColumn(\"rowCol\", row_number().over(Window.partitionBy(\"item_type\").orderBy(\"item_identifier\"))) # Groepen gebaseerd op item type, geoordend op item identifier krijgen rij nummer op volgorde\n",
    "\n",
    "# Rank() en dense rank()\n",
    "df_csv.withColumn(\"rank\", rank()\\\n",
    "    .over(Window.partitionBy(\"item_type\").orderBy(\"item_mrp\"))) # 1 5 6 9\n",
    "df_csv.withColumn(\"denseRank\", dense_rank()\\\n",
    "    .over(Window.partitionBy(\"item_type\").orderBy(col(\"item_mrp\").desc()))) # 1 2 3 4\n",
    "\n",
    "# Cumulative sum\n",
    "df_csv.withColumn(\"csum\", sum(\"item_mrp\")\\\n",
    "    .over(Window.partitionBy(\"item_type\").orderBy(\"item_type\").rowsBetween(Window.unboundedPreceding, Window.currentRow))) # telt alle item_mrp op per item type geordern op item type pakt alle rijen ervoor en telt de huidige erbij op.\n",
    "\n",
    "df_csv.withColumn(\"csum\", sum(\"item_mrp\")\\\n",
    "    .over(Window.partitionBy(\"item_type\").orderBy(\"item_type\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))) # Welke rijen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ea207c3-18fe-4e74-b0c7-f1c3bbf4e83d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a10b83-446b-458f-8b1d-dd527ac06c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Is niet efficient needs python intrepeter, dus traag. Alleen als er geen workaround is. \n",
    "# Define function\n",
    "def my_function(x):\n",
    "    return x + 1\n",
    "\n",
    "# Step 2 \n",
    "my_udf = udf(my_function)\n",
    "\n",
    "# step 3 apply\n",
    "df_csv.withColumn(\"new_col\", my_udf(col(\"item_mrp\")))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Training_pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}